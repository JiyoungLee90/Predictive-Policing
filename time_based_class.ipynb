{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Based Validation \n",
    "\n",
    "This class has:\n",
    "\n",
    "- sliding_window method\n",
    "\n",
    "- forward_chaining method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime\n",
    "\n",
    "class TimeBasedCV():\n",
    "    '''\n",
    "    This class is inspired from:\n",
    "    https://gist.github.com/orhermansaffar/2bd2342c81026de1c09c97d66226eb46#file-timebasedcv-py\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    - train_period: int\n",
    "      number of time units to include in each train set for sliding window method\n",
    "      number of time units to initally include in first train set for forward chaining method\n",
    "      default is 3(months)\n",
    "        \n",
    "    - test_period: int\n",
    "      number of time units to include in each test set\n",
    "      default is 1(month)\n",
    "      \n",
    "    - freq: string\n",
    "        frequency of input parameters. possible values are: days, months, years, weeks, hours, minutes, seconds\n",
    "        possible values designed to be used by dateutil.relativedelta class\n",
    "        deafault is months\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, train_period = 3, test_period = 1, freq = 'months'):\n",
    "        self.train_period = train_period\n",
    "        self.test_period = test_period\n",
    "        self.freq = freq\n",
    "        \n",
    "        \n",
    "    def sliding_window(self, data, first_split_date = None, date_column = 'Month', gap = 0):\n",
    "        \n",
    "        '''\n",
    "         Generate indices using sliding window method to split data into training and test set\n",
    "         \n",
    "         Parameters:\n",
    "         \n",
    "        - data: pandas DataFrame\n",
    "          the data contains date column\n",
    "            \n",
    "        - first_split_date: datetime.date()\n",
    "          first date to perform the splitting on.\n",
    "          if not provided will set to be the minimum date in the data after first training set\n",
    "        \n",
    "        - date_column: string, default = 'Month'\n",
    "          date of each record\n",
    "            \n",
    "        - gap: int, default = 0\n",
    "          for cases the test set does not come right after the train set, \n",
    "          *gap* days are left between train and test sets\n",
    "            \n",
    "            \n",
    "        Returns\n",
    "        \n",
    "        - train_index, test_index:\n",
    "          list of tuples(train index, test index)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # check if date_column exist in the data:\n",
    "        \n",
    "        try: \n",
    "            data[date_column]\n",
    "        except:\n",
    "            raise KeyError(date_column)\n",
    "            \n",
    "        train_indices_list = []\n",
    "        test_indices_list = []\n",
    "        \n",
    "        if first_split_date == None:\n",
    "            first_split_date = data[date_column].min().date() + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "            \n",
    "        start_train = first_split_date - eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "        end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "        \n",
    "        while end_test <= data[date_column].max().date() + eval('relativedelta('+self.freq+'=1)'):\n",
    "            # train indices:\n",
    "            cur_train_indices = list(data[(data[date_column].dt.date>=start_train) & \n",
    "                                     (data[date_column].dt.date<end_train)].index)\n",
    "\n",
    "            # test indices:\n",
    "            cur_test_indices = list(data[(data[date_column].dt.date>=start_test) &\n",
    "                                    (data[date_column].dt.date<end_test)].index)\n",
    "            \n",
    "            print(\"Train period:\",start_train,\"-\" , end_train, \", Test period\", start_test, \"-\", end_test)\n",
    "                  #\"# train records\", len(cur_train_indices), \", # test records\", len(cur_test_indices))\n",
    "\n",
    "            train_indices_list.append(cur_train_indices)\n",
    "            test_indices_list.append(cur_test_indices)\n",
    "\n",
    "            # update dates:\n",
    "            start_train = start_train + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "            end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "            start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "            end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "\n",
    "        \n",
    "        index_output = [(train, test) for train, test in zip(train_indices_list,test_indices_list)]\n",
    "\n",
    "        self.n_splits = len(index_output)\n",
    "        \n",
    "        return index_output\n",
    "    \n",
    "    \n",
    "    def forward_chaining(self, data, first_split_date = None, date_column = 'Month', gap = 0):\n",
    "        \n",
    "        try:\n",
    "            data[date_column]\n",
    "        except:\n",
    "            raise KeyError(date_column)\n",
    "        \n",
    "        train_indices_list = []\n",
    "        test_indices_list = []\n",
    "        \n",
    "        if first_split_date == None:\n",
    "            first_split_date = data[date_column].min().date() + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "            \n",
    "        start_train = first_split_date - eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        end_train = start_train + eval('relativedelta('+self.freq+'=self.train_period)')\n",
    "        start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "        end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')   \n",
    "        \n",
    "        while end_test <= data[date_column].max().date() + eval('relativedelta('+self.freq+'=1)'):\n",
    "            cur_train_indices = list(data[(data[date_column].dt.date>=start_train) & \n",
    "                                     (data[date_column].dt.date<end_train)].index)\n",
    "            \n",
    "            cur_test_indices = list(data[(data[date_column].dt.date>=start_test) &\n",
    "                                    (data[date_column].dt.date<end_test)].index)\n",
    "            \n",
    "            print(\"Train period:\",start_train,\"-\" , end_train, \", Test period\", start_test, \"-\", end_test)\n",
    "                  #\"# train records\", len(cur_train_indices), \", # test records\", len(cur_test_indices))\n",
    "            \n",
    "            train_indices_list.append(cur_train_indices)\n",
    "            test_indices_list.append(cur_test_indices)\n",
    "            \n",
    "            # update dates:\n",
    "            start_train = start_train \n",
    "            end_train = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "            start_test = end_train + eval('relativedelta('+self.freq+'=gap)')\n",
    "            end_test = start_test + eval('relativedelta('+self.freq+'=self.test_period)')\n",
    "        \n",
    "        index_output = [(train, test) for train, test in zip(train_indices_list,test_indices_list)]\n",
    "        \n",
    "        self.n_splits = len(index_output)\n",
    "        \n",
    "        return index_output\n",
    "        \n",
    "        \n",
    "    def get_n_splits(self):\n",
    "        \n",
    "        '''\n",
    "        Returns the number of splitting iterations in the cross-validator\n",
    "        \n",
    "        Returns\n",
    "        \n",
    "        - n_splits : int\n",
    "          Returns the number of splitting iterations in the cross-validator.\n",
    "          \n",
    "        '''\n",
    "        return self.n_splits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Based Target Encoding \n",
    "target encoding using time based validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import category_encoders.utils as util\n",
    "import numpy as np\n",
    "\n",
    "class SWTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    '''\n",
    "    Target encoding for categorical features using sliding window method\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    - col_names: list\n",
    "      a list of columns to encode, if None, all string columns will be encoded.\n",
    "    \n",
    "    - target_name: string\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, target_name, col_names = None,):\n",
    "        self.col_names = col_names\n",
    "        self.target_name = target_name\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y = None, drop_target = True, drop_na = True, drop_cat = True):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, drop_target = True, drop_na = True, drop_cat = True):\n",
    "        \n",
    "        # if columns aren't passed, just use every string column\n",
    "        if self.col_names is None:\n",
    "            self.col_names = util.get_obj_cols(X)\n",
    "        else:\n",
    "            self.col_names = util.convert_cols_to_list(self.col_names)\n",
    "        \n",
    "        assert type(self.target_name) == str, \"target_name must be a string\"\n",
    "        assert type(self.col_names) == list, \"col_names must be a list\"\n",
    "        assert set(self.col_names).issubset(set(X.columns)), \"column name is not found in the dataframe\"\n",
    "        assert self.target_name in X.columns, \"target_name is not found in the dataframe\"\n",
    "        \n",
    "        target_mean = X[self.target_name].mean()\n",
    "        \n",
    "        # split X using sliding window method and get indices\n",
    "        time_cv = SlidingWindow(train_period = 1, test_period = 1)\n",
    "        sliding_window_indices = time_cv.split(X)\n",
    "        \n",
    "        for col in self.col_names:\n",
    "            col_mean_name = col + '_' + 'target_mean'\n",
    "            X[col_mean_name] = np.nan\n",
    "            \n",
    "            for train_indices, valid_indices in sliding_window_indices:\n",
    "                X_train, X_valid = X.loc[train_indices], X.loc[valid_indices]\n",
    "                X.loc[valid_indices, col_mean_name] = X_valid[col].map(X_train.groupby(col)[self.target_name].mean())\n",
    "        \n",
    "        # drop the rows having NaN values (not encoded)\n",
    "        if drop_na == True:\n",
    "            X.dropna(inplace = True)\n",
    "        \n",
    "        # drop target\n",
    "        if drop_target == True:\n",
    "            X.drop(self.target_name, axis = 1, inplace = True)\n",
    "        \n",
    "        # drop categorical columns\n",
    "        if drop_cat == True:\n",
    "            X.drop([cols for cols in self.col_names], axis = 1, inplace = True)\n",
    "        \n",
    "        return X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import category_encoders.utils as util\n",
    "import numpy as np\n",
    "\n",
    "class FCTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    '''\n",
    "    Target encoding for categorical features using forward chaining method\n",
    "    \n",
    "    Parameters\n",
    "    \n",
    "    - col_names: list\n",
    "      a list of columns to encode, if None, all string columns will be encoded.\n",
    "    \n",
    "    - target_name: string\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, target_name, col_names = None):\n",
    "        self.target_name = target_name\n",
    "        self.col_names = col_names\n",
    "      \n",
    "    def fit(self, X, y = None, drop_target = True, drop_na = True, drop_cat = True):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, drop_target = True, drop_na = True, drop_cat = True):\n",
    "        \n",
    "        # if columns aren't passed, just use every string column\n",
    "        if self.col_names is None:\n",
    "            self.col_names = util.get_obj_cols(X)\n",
    "        else:\n",
    "            self.col_names = util.convert_cols_to_list(self.col_names)\n",
    "        \n",
    "        assert type(self.target_name) == str, \"target_name must be a string\"\n",
    "        assert type(self.col_names) == list, \"col_names must be a list\"\n",
    "        assert set(self.col_names).issubset(set(X.columns)), \"column name is not found in the dataframe\"\n",
    "        assert self.target_name in X.columns, \"target_name is not found in the dataframe\"\n",
    "        \n",
    "        target_mean = X[self.target_name].mean()\n",
    "        \n",
    "        # split X using  forward chaining method and get indices\n",
    "        time_cv = TimeBasedCV(train_period = 1, test_period = 1)\n",
    "        forward_chaining_indices = time_cv.forward_chaining(X) \n",
    "        \n",
    "        for col in self.col_names:\n",
    "            col_mean_name = col + '_' + 'target_mean'\n",
    "            X[col_mean_name] = np.nan\n",
    "            \n",
    "            for train_indices, valid_indices in forward_chaining_indices:\n",
    "                X_train, X_valid = X.loc[train_indices], X.loc[valid_indices]\n",
    "                X.loc[valid_indices, col_mean_name] = X_valid[col].map(X_train.groupby(col)[self.target_name].mean()\n",
    "                                                                      )\n",
    "        # drop the rows having NaN values (not encoded)\n",
    "        if drop_na == True:\n",
    "            X.dropna(inplace = True)\n",
    "        \n",
    "        #drop target\n",
    "        if drop_target == True:\n",
    "            X.drop(self.target_name, axis = 1, inplace = True)\n",
    "        \n",
    "        # drop categorical columns\n",
    "        if drop_cat == True:\n",
    "            X.drop([cols for cols in self.col_names], axis = 1, inplace = True)\n",
    "            \n",
    "        return X \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
